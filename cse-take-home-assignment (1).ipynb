{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbfaad8-42d7-4146-b6ca-f153b1507f92"}}},{"cell_type":"markdown","source":["# CSE Coding Assignment\n## Instructions\n\n- Please answer all questions\n- You can use any language you wish (e.g. Python, Scala, SQL...)\n- Several Markdown cells require completion. Please edit the Markdown cells to include your answer.\n- Your final notebook should compile without errors when you click \"Run All\"\n\n**Please do not publish questions. This is a confidential assignment.**\n\n### Creating a Cluster\n\nYou will need to create a Databricks Cluster. More information on this process is available here: https://docs.databricks.com/user-guide/clusters/create.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e6aa93-4d81-4cfb-82f4-cb24e5c5b33c"}}},{"cell_type":"markdown","source":["## Getting Started\n\n**REQUIRED:** Run the following cells exactly as written to retrieve the necessary Coding Assignment Data Sets from Amazon S3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af916113-b24d-4b0c-8543-ccefb6094c48"}}},{"cell_type":"code","source":["%sh curl --remote-name-all 'https://files.training.databricks.com/assessments/cse-take-home/{covertype,kafka,treecover,u.data,u.item}.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4622ef0-0774-40b6-a490-8464619c3f4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n[1/5]: https://files.training.databricks.com/assessments/cse-take-home/covertype.csv --> covertype.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   133  100   133    0     0    773      0 --:--:-- --:--:-- --:--:--   773\n\n[2/5]: https://files.training.databricks.com/assessments/cse-take-home/kafka.csv --> kafka.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  135k  100  135k    0     0   863k      0 --:--:-- --:--:-- --:--:--  863k\n\n[3/5]: https://files.training.databricks.com/assessments/cse-take-home/treecover.csv --> treecover.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  642k  100  642k    0     0  3712k      0 --:--:-- --:--:-- --:--:-- 3712k\n\n[4/5]: https://files.training.databricks.com/assessments/cse-take-home/u.data.csv --> u.data.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 1932k  100 1932k    0     0  12.6M      0 --:--:-- --:--:-- --:--:-- 12.6M\n\n[5/5]: https://files.training.databricks.com/assessments/cse-take-home/u.item.csv --> u.item.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n 53  230k   53  122k    0     0   689k      0 --:--:-- --:--:-- --:--:--  685k\n100  230k  100  230k    0     0   905k      0 --:--:-- --:--:-- --:--:--  901k\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/covertype.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/kafka.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/treecover.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.data.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.item.csv\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\n[1/5]: https://files.training.databricks.com/assessments/cse-take-home/covertype.csv --> covertype.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   133  100   133    0     0    773      0 --:--:-- --:--:-- --:--:--   773\n\n[2/5]: https://files.training.databricks.com/assessments/cse-take-home/kafka.csv --> kafka.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  135k  100  135k    0     0   863k      0 --:--:-- --:--:-- --:--:--  863k\n\n[3/5]: https://files.training.databricks.com/assessments/cse-take-home/treecover.csv --> treecover.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  642k  100  642k    0     0  3712k      0 --:--:-- --:--:-- --:--:-- 3712k\n\n[4/5]: https://files.training.databricks.com/assessments/cse-take-home/u.data.csv --> u.data.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 1932k  100 1932k    0     0  12.6M      0 --:--:-- --:--:-- --:--:-- 12.6M\n\n[5/5]: https://files.training.databricks.com/assessments/cse-take-home/u.item.csv --> u.item.csv\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n 53  230k   53  122k    0     0   689k      0 --:--:-- --:--:-- --:--:--  685k\n100  230k  100  230k    0     0   905k      0 --:--:-- --:--:-- --:--:--  901k\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/covertype.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/kafka.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/treecover.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.data.csv\n--_curl_--https://files.training.databricks.com/assessments/cse-take-home/u.item.csv\n"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/databricks/driver/covertype.csv\", \"dbfs:/FileStore/tmp/covertype.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/kafka.csv\", \"dbfs:/FileStore/tmp/kafka.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/treecover.csv\", \"dbfs:/FileStore/tmp/treecover.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.data.csv\", \"dbfs:/FileStore/tmp/u.data.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.item.csv\", \"dbfs:/FileStore/tmp/u.item.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e050ea2d-e539-4efa-aa18-813102bbe3e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 1: Reading and Parsing Data\n\n### Question 1:  Code Challenge - Load a CSV\n\n- Load the CSV file at `dbfs:/FileStore/tmp/nl/treecover.csv` into a DataFrame.\n- Use Apache Spark to read in the data, assigned to the variable `treeCoverDF`.\n- Your method to get the CSV file into Databricks isn't graded. We are only concerned with how you use Spark to parse and load the actual data. \n- Please use the `inferSchema` option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6017a5eb-8c81-460e-902d-2b69588a7bd5"}}},{"cell_type":"code","source":["# YOUR CODE HERE\ntreeCoverDF=spark.read.csv(\"/FileStore/tmp/treecover.csv\",inferSchema=\"true\",header=\"true\")\ntreeCoverDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58e3f180-5eed-4f10-838f-f96408365dcc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Horizontal_Distance_To_Fire_Points|Cover_Type|Soil_Type|Wilderness_Area|Hillshade|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n|  1|     2596|    51|    3|                             258|                             0|                            510|                              6279|         5|       29|              1|      9am|\n|  2|     2590|    56|    2|                             212|                            -6|                            390|                              6225|         5|       29|              1|     Noon|\n|  3|     2804|   139|    9|                             268|                            65|                           3180|                              6121|         2|       12|              1|      3pm|\n|  4|     2785|   155|   18|                             242|                           118|                           3090|                              6211|         2|       30|              1|      9am|\n|  5|     2595|    45|    2|                             153|                            -1|                            391|                              6172|         5|       29|              1|     Noon|\n|  6|     2579|   132|    6|                             300|                           -15|                             67|                              6031|         2|       29|              1|      3pm|\n|  7|     2606|    45|    7|                             270|                             5|                            633|                              6256|         5|       29|              1|      9am|\n|  8|     2605|    49|    4|                             234|                             7|                            573|                              6228|         5|       29|              1|     Noon|\n|  9|     2617|    45|    9|                             240|                            56|                            666|                              6244|         5|       29|              1|      3pm|\n| 10|     2612|    59|   10|                             247|                            11|                            636|                              6230|         5|       29|              1|      9am|\n| 11|     2612|   201|    4|                             180|                            51|                            735|                              6222|         5|       18|              1|     Noon|\n| 12|     2886|   151|   11|                             371|                            26|                           5253|                              4051|         2|       30|              1|      3pm|\n| 13|     2742|   134|   22|                             150|                            69|                           3215|                              6091|         2|       30|              1|      9am|\n| 14|     2609|   214|    7|                             150|                            46|                            771|                              6211|         5|       18|              1|     Noon|\n| 15|     2503|   157|    4|                              67|                             4|                            674|                              5600|         5|       18|              1|      3pm|\n| 16|     2495|    51|    7|                              42|                             2|                            752|                              5576|         5|       16|              1|      9am|\n| 17|     2610|   259|    1|                             120|                            -1|                            607|                              6096|         5|       29|              1|     Noon|\n| 18|     2517|    72|    7|                              85|                             6|                            595|                              5607|         5|       18|              1|      3pm|\n| 19|     2504|     0|    4|                              95|                             5|                            691|                              5572|         5|       18|              1|      9am|\n| 20|     2503|    38|    5|                              85|                            10|                            741|                              5555|         5|       18|              1|     Noon|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Horizontal_Distance_To_Fire_Points|Cover_Type|Soil_Type|Wilderness_Area|Hillshade|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n|  1|     2596|    51|    3|                             258|                             0|                            510|                              6279|         5|       29|              1|      9am|\n|  2|     2590|    56|    2|                             212|                            -6|                            390|                              6225|         5|       29|              1|     Noon|\n|  3|     2804|   139|    9|                             268|                            65|                           3180|                              6121|         2|       12|              1|      3pm|\n|  4|     2785|   155|   18|                             242|                           118|                           3090|                              6211|         2|       30|              1|      9am|\n|  5|     2595|    45|    2|                             153|                            -1|                            391|                              6172|         5|       29|              1|     Noon|\n|  6|     2579|   132|    6|                             300|                           -15|                             67|                              6031|         2|       29|              1|      3pm|\n|  7|     2606|    45|    7|                             270|                             5|                            633|                              6256|         5|       29|              1|      9am|\n|  8|     2605|    49|    4|                             234|                             7|                            573|                              6228|         5|       29|              1|     Noon|\n|  9|     2617|    45|    9|                             240|                            56|                            666|                              6244|         5|       29|              1|      3pm|\n| 10|     2612|    59|   10|                             247|                            11|                            636|                              6230|         5|       29|              1|      9am|\n| 11|     2612|   201|    4|                             180|                            51|                            735|                              6222|         5|       18|              1|     Noon|\n| 12|     2886|   151|   11|                             371|                            26|                           5253|                              4051|         2|       30|              1|      3pm|\n| 13|     2742|   134|   22|                             150|                            69|                           3215|                              6091|         2|       30|              1|      9am|\n| 14|     2609|   214|    7|                             150|                            46|                            771|                              6211|         5|       18|              1|     Noon|\n| 15|     2503|   157|    4|                              67|                             4|                            674|                              5600|         5|       18|              1|      3pm|\n| 16|     2495|    51|    7|                              42|                             2|                            752|                              5576|         5|       16|              1|      9am|\n| 17|     2610|   259|    1|                             120|                            -1|                            607|                              6096|         5|       29|              1|     Noon|\n| 18|     2517|    72|    7|                              85|                             6|                            595|                              5607|         5|       18|              1|      3pm|\n| 19|     2504|     0|    4|                              95|                             5|                            691|                              5572|         5|       18|              1|      9am|\n| 20|     2503|    38|    5|                              85|                            10|                            741|                              5555|         5|       18|              1|     Noon|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 2:  Code Challenge - Print the Schema\n\nUse Apache Spark to display the Schema of the `treeCoverDF` Dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7507f053-7a27-4422-aa1a-86dd75037f1b"}}},{"cell_type":"code","source":["treeCoverDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d817e77-2d16-4f02-80be-e80af133ca82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Id: integer (nullable = true)\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: integer (nullable = true)\n |-- Soil_Type: integer (nullable = true)\n |-- Wilderness_Area: integer (nullable = true)\n |-- Hillshade: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Id: integer (nullable = true)\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: integer (nullable = true)\n |-- Soil_Type: integer (nullable = true)\n |-- Wilderness_Area: integer (nullable = true)\n |-- Hillshade: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 3:  Code Challenge - Rows & Columns\n\nUse Apache Spark to display the number of rows and columns in the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e44ffca2-82f1-43ea-b873-912674165b67"}}},{"cell_type":"code","source":["print(\"Rows:\",treeCoverDF.count())\nprint(\"Columns:\",len(treeCoverDF.columns))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84ca332b-64b2-4608-98ad-95dc7a0925a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Rows: 15120\nColumns: 12\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Rows: 15120\nColumns: 12\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Part 2: Analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c4e7ed-31f5-4ec2-b8a0-95b505985c35"}}},{"cell_type":"markdown","source":["### Question 4:  Code Challenge - Summary Statistics for a Feature\n\nUse Apache Spark to answer these questions about the `treeCoverDF` DataFrame:\n- What is the range - minimum and maximum - of values for the feature `elevation`?\n- What are the mean and standard deviation of the feature `elevation`?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecb0ee4-834c-484c-9a8d-a1763cb22109"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ntreeCoverDF.agg(min(col('elevation')),max(col('elevation'))).show()\ntreeCoverDF.agg(mean(col('elevation')),stddev(col('elevation'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a58613-9c3c-46fe-8a43-7cbcc8df2875"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+--------------+\n|min(elevation)|max(elevation)|\n+--------------+--------------+\n|          1863|          3849|\n+--------------+--------------+\n\n+------------------+----------------------+\n|    avg(elevation)|stddev_samp(elevation)|\n+------------------+----------------------+\n|2749.3225529100528|    417.67818734804985|\n+------------------+----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+--------------+\n|min(elevation)|max(elevation)|\n+--------------+--------------+\n|          1863|          3849|\n+--------------+--------------+\n\n+------------------+----------------------+\n|    avg(elevation)|stddev_samp(elevation)|\n+------------------+----------------------+\n|2749.3225529100528|    417.67818734804985|\n+------------------+----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Answer #4:\n\n- Min `elevation`: `1863`\n- Max `elevation`: `3849`\n- Mean `elevation`: `2749`\n- Standard Deviation of `elevation`: `417`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eacc2888-9f7d-4d36-9e08-d0eac0808c4e"}}},{"cell_type":"markdown","source":["### Question 5:  Code Challenge - Record Count\n\nUse Apache Spark to answer the following question:\n- How many entries in the dataset have an `elevation` greater than or equal to 2749.32 meters **AND** a `Cover_Type` of 1 or 2?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e94fccb6-891e-4092-9d46-8b32a9265b16"}}},{"cell_type":"code","source":["treeCoverDF.filter('elevation >= 2749.32 and cover_type =1 or cover_type = 2').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ac85423-80db-45d8-b580-609d3a6ef929"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Horizontal_Distance_To_Fire_Points|Cover_Type|Soil_Type|Wilderness_Area|Hillshade|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n|  3|     2804|   139|    9|                             268|                            65|                           3180|                              6121|         2|       12|              1|      3pm|\n|  4|     2785|   155|   18|                             242|                           118|                           3090|                              6211|         2|       30|              1|      9am|\n|  6|     2579|   132|    6|                             300|                           -15|                             67|                              6031|         2|       29|              1|      3pm|\n| 12|     2886|   151|   11|                             371|                            26|                           5253|                              4051|         2|       30|              1|      3pm|\n| 13|     2742|   134|   22|                             150|                            69|                           3215|                              6091|         2|       30|              1|      9am|\n| 22|     2880|   209|   17|                             216|                            30|                           4986|                              4323|         2|       30|              1|      9am|\n| 28|     2962|   148|   16|                             323|                            23|                           5916|                              3395|         2|       29|              1|      9am|\n| 29|     2811|   135|    1|                             212|                            30|                           3670|                              5643|         2|       12|              1|     Noon|\n| 36|     2900|    45|   19|                             242|                            20|                           5199|                              4115|         2|       29|              1|      3pm|\n| 42|     2570|   346|    2|                               0|                             0|                            331|                              5745|         2|       29|              1|      3pm|\n| 45|     2678|   128|    5|                              95|                            23|                           1660|                              6546|         2|       12|              1|      3pm|\n| 62|     2952|   107|   11|                              42|                             7|                           5845|                              3509|         2|       29|              1|     Noon|\n| 63|     2705|    90|    8|                             134|                            22|                           2023|                              6615|         2|       12|              1|      3pm|\n| 68|     2919|    13|   13|                              90|                             6|                           5321|                              4060|         1|       29|              1|     Noon|\n| 69|     2740|    54|    6|                             218|                            42|                           2287|                              6686|         2|       12|              1|      3pm|\n| 70|     2640|    80|    8|                             180|                            -2|                           1092|                              5866|         2|       29|              1|      9am|\n| 71|     2843|   166|   12|                             242|                            53|                           4434|                              4956|         2|       30|              1|     Noon|\n| 72|     3008|    45|   14|                             277|                            10|                           6371|                              3036|         2|       29|              1|      3pm|\n| 73|     2893|   114|   16|                             108|                            30|                           5066|                              4340|         2|       29|              1|      9am|\n| 74|     2850|     6|    9|                               0|                             0|                           4858|                              4548|         2|       16|              1|     Noon|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Horizontal_Distance_To_Fire_Points|Cover_Type|Soil_Type|Wilderness_Area|Hillshade|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n|  3|     2804|   139|    9|                             268|                            65|                           3180|                              6121|         2|       12|              1|      3pm|\n|  4|     2785|   155|   18|                             242|                           118|                           3090|                              6211|         2|       30|              1|      9am|\n|  6|     2579|   132|    6|                             300|                           -15|                             67|                              6031|         2|       29|              1|      3pm|\n| 12|     2886|   151|   11|                             371|                            26|                           5253|                              4051|         2|       30|              1|      3pm|\n| 13|     2742|   134|   22|                             150|                            69|                           3215|                              6091|         2|       30|              1|      9am|\n| 22|     2880|   209|   17|                             216|                            30|                           4986|                              4323|         2|       30|              1|      9am|\n| 28|     2962|   148|   16|                             323|                            23|                           5916|                              3395|         2|       29|              1|      9am|\n| 29|     2811|   135|    1|                             212|                            30|                           3670|                              5643|         2|       12|              1|     Noon|\n| 36|     2900|    45|   19|                             242|                            20|                           5199|                              4115|         2|       29|              1|      3pm|\n| 42|     2570|   346|    2|                               0|                             0|                            331|                              5745|         2|       29|              1|      3pm|\n| 45|     2678|   128|    5|                              95|                            23|                           1660|                              6546|         2|       12|              1|      3pm|\n| 62|     2952|   107|   11|                              42|                             7|                           5845|                              3509|         2|       29|              1|     Noon|\n| 63|     2705|    90|    8|                             134|                            22|                           2023|                              6615|         2|       12|              1|      3pm|\n| 68|     2919|    13|   13|                              90|                             6|                           5321|                              4060|         1|       29|              1|     Noon|\n| 69|     2740|    54|    6|                             218|                            42|                           2287|                              6686|         2|       12|              1|      3pm|\n| 70|     2640|    80|    8|                             180|                            -2|                           1092|                              5866|         2|       29|              1|      9am|\n| 71|     2843|   166|   12|                             242|                            53|                           4434|                              4956|         2|       30|              1|     Noon|\n| 72|     3008|    45|   14|                             277|                            10|                           6371|                              3036|         2|       29|              1|      3pm|\n| 73|     2893|   114|   16|                             108|                            30|                           5066|                              4340|         2|       29|              1|      9am|\n| 74|     2850|     6|    9|                               0|                             0|                           4858|                              4548|         2|       16|              1|     Noon|\n+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 6: Code Challenge - Compute a Percentage\n\nUse Apache Spark to answer the following question:\n- What percentage of entries with `Cover_Type` 1 or 2 have an `elevation` at or above 2749.32 meters?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51ff1431-1305-46fc-a59a-7b2378770d4a"}}},{"cell_type":"code","source":["filter_count=treeCoverDF.filter('elevation >=2749.32 and cover_type = 1 or cover_type = 2').count()\ntotal_rec=treeCoverDF.count()\nPercentage=(filter_count*100)/total_rec\nprint(Percentage)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ec8a16b-d4e7-4498-85cf-abe310618b2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"28.30026455026455\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["28.30026455026455\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 7: Code Challenge - Visualize Feature Distribution\n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart that helps visualize the distribution of different Wilderness Areas in our dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed1a9f4-703b-4332-b31f-bcea9ad80b54"}}},{"cell_type":"code","source":["display(treeCoverDF.select('wilderness_area').distinct())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a28563-31ab-4e56-8730-4d9a1ede84d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1],[3],[4],[2]],"plotOptions":{"displayType":"plotlyBar","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"wilderness_area","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>wilderness_area</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 8: Code Challenge - Visualize Average Elevation by Cover Type \n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart showing the average elevation of each cover type with string labels for cover type\n\n**NOTE: you will need to match the integer values in the column `treeCoverDF.Cover_Type` to the string values in `dbfs:/FileStore/tmp/nl/covertype.csv` to retrieve the Cover Type Labels. It is recommended to use an Apache Spark join.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"821447ca-4ff7-448a-9cad-10691d55bef6"}}},{"cell_type":"code","source":["Cover_type=spark.read.csv(\"dbfs:/FileStore/tmp/covertype.csv\",header=\"true\", inferSchema=\"true\")\nCover_type.show()\nnewdf=treeCoverDF.join(Cover_type,treeCoverDF.Cover_Type==Cover_type.cover_type_key)\ndisplay(newdf.select('elevation','cover_type_label'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"992c86ce-502a-4f58-967f-023813bfbd9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+-----------------+\n|cover_type_key| cover_type_label|\n+--------------+-----------------+\n|             1|       Spruce/Fir|\n|             2|   Lodgepole Pine|\n|             3|   Ponderosa Pine|\n|             4|Cottonwood/Willow|\n|             5|            Aspen|\n|             6|      Douglas-fir|\n|             7|        Krummholz|\n+--------------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+-----------------+\n|cover_type_key| cover_type_label|\n+--------------+-----------------+\n|             1|       Spruce/Fir|\n|             2|   Lodgepole Pine|\n|             3|   Ponderosa Pine|\n|             4|Cottonwood/Willow|\n|             5|            Aspen|\n|             6|      Douglas-fir|\n|             7|        Krummholz|\n+--------------+-----------------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3725234820062639>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mCover_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/FileStore/tmp/covertype.csv\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"true\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minferSchema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mCover_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mnewdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCover_type\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCover_Type\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mCover_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcover_type_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'elevation'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'cover_type_label'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'treeCoverDF' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'treeCoverDF' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3725234820062639>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mCover_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/FileStore/tmp/covertype.csv\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"true\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minferSchema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mCover_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mnewdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCover_type\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtreeCoverDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCover_Type\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mCover_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcover_type_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'elevation'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'cover_type_label'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'treeCoverDF' is not defined"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Part 3: Data Ingestion, Cleansing, and Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9c08c2-633c-48f4-88ce-64fef0946c02"}}},{"cell_type":"markdown","source":["## Instructions \n\nThis is a multi-step, data pipeline question in which you need to achieve a few objectives to build a successful job.\n\n### Data Sets\n\n#### `u.data.csv`\n\n- The full u data set, 100000 ratings by 943 users on 1682 items. \n- Each user has rated at least 20 movies.  \n- Users and items are numbered consecutively from 1. \n- The data is randomly ordered. \n- This is a tab separated file consisting of four columns: \n   - user id \n   - movie id \n   - rating \n   - date (unix seconds since 1/1/1970 UTC)\n\n#### Desired schema\n\n- `user_id INTEGER`\n- `movie_id INTEGER`\n- `rating INTEGER`\n- `date DATE `\n\n#### `u.item.csv`\n\n- This is a `|` separated file consisting of six columns:\n   - movie id\n   - movie title\n   - release date\n   - video release date\n   - IMDb URL\n   - genre\n- movie ids in this file match movie ids in `u.data`.\n\n#### Desired schema\n\n- `movie_id INTEGER`\n- `movie_title STRING`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7324d228-43d3-4eb8-8624-35fe5a11be26"}}},{"cell_type":"markdown","source":["### Question 9:  Code Challenge - Load DataFrames\n\nUse Apache Spark to perform the following:\n1. define the correct schemas for each Data Set to be imported as described above  \n   **note:** \n      - for `u.data.csv`, `date` *must* be stored using `DateType` with the format `yyyy-MM-dd`\n      - you may need to ingest `timestamp` data using `IntegerType`\n      - be sure to drop unneccesary columns for `u.item.csv`\n1. import the two files as DataFrames names `uDataDF` and `uItemDF` using the schemas you defined and these paths:\n   - `dbfs:/FileStore/tmp/u.data.csv`\n   - `dbfs:/FileStore/tmp/u.item.csv`\n1. order the `uDataDF` DataFrame by the `date` column\n\n**NOTE:** Please display the DataFrames, `uDataDF` and `uItemDF` after loading."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83b2d516-a0ea-4665-a654-fb2da2e26b18"}}},{"cell_type":"markdown","source":["#### `uDataDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7e6125-08ad-4431-85bc-63a94c70e3e0"}}},{"cell_type":"code","source":["from pyspark.sql.functions  import date_format\n\nuDataDF=spark.read.load(\"dbfs:/FileStore/tmp/u.data.csv\",format=\"csv\",sep=\"\\t\",header=\"false\",inferSchema=\"true\")\n\nuDataDF = uDataDF.withColumnRenamed(\"_c0\", \"user_id\").withColumnRenamed(\"_c1\", \"movie_id\").withColumnRenamed(\"_c2\", \"rating\").withColumnRenamed(\"_c3\", \"date\").withColumn(\"tsDate\",from_unixtime(\"date\")).withColumn(\"newdate\",date_format('tsDate', \"yyyy-MM-dd\")).drop('tsDate','date')\n\nuDataDF.sort(desc(\"newdate\")).show()\n\n#from_unixtime(col(\"timestamp_3\"),\"MM-dd-yyyy\").alias(\"timestamp_3\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb26c967-2f8f-4f44-88e4-889760214a11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+--------+------+----------+\n|user_id|movie_id|rating|   newdate|\n+-------+--------+------+----------+\n|    189|      30|     4|1998-04-22|\n|    189|     496|     5|1998-04-22|\n|    189|     120|     1|1998-04-22|\n|    189|     520|     5|1998-04-22|\n|    189|     462|     5|1998-04-22|\n|    189|     526|     4|1998-04-22|\n|    159|     259|     4|1998-04-22|\n|    189|      20|     5|1998-04-22|\n|    189|     197|     5|1998-04-22|\n|    189|     694|     4|1998-04-22|\n|    189|      45|     3|1998-04-22|\n|    189|     751|     4|1998-04-22|\n|    189|     276|     3|1998-04-22|\n|    189|    1115|     4|1998-04-22|\n|    189|     253|     4|1998-04-22|\n|    189|       1|     5|1998-04-22|\n|    189|     480|     5|1998-04-22|\n|    189|    1060|     5|1998-04-22|\n|    189|     705|     4|1998-04-22|\n|    189|      91|     3|1998-04-22|\n+-------+--------+------+----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+--------+------+----------+\n|user_id|movie_id|rating|   newdate|\n+-------+--------+------+----------+\n|    189|      30|     4|1998-04-22|\n|    189|     496|     5|1998-04-22|\n|    189|     120|     1|1998-04-22|\n|    189|     520|     5|1998-04-22|\n|    189|     462|     5|1998-04-22|\n|    189|     526|     4|1998-04-22|\n|    159|     259|     4|1998-04-22|\n|    189|      20|     5|1998-04-22|\n|    189|     197|     5|1998-04-22|\n|    189|     694|     4|1998-04-22|\n|    189|      45|     3|1998-04-22|\n|    189|     751|     4|1998-04-22|\n|    189|     276|     3|1998-04-22|\n|    189|    1115|     4|1998-04-22|\n|    189|     253|     4|1998-04-22|\n|    189|       1|     5|1998-04-22|\n|    189|     480|     5|1998-04-22|\n|    189|    1060|     5|1998-04-22|\n|    189|     705|     4|1998-04-22|\n|    189|      91|     3|1998-04-22|\n+-------+--------+------+----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### `uItemDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46675f81-dc38-4b36-8621-bb5a1956c54b"}}},{"cell_type":"code","source":["uItemDF=spark.read.load(\"dbfs:/FileStore/tmp/u.item.csv\",format=\"csv\",sep=\"|\",header=\"false\",inferSchema=\"true\")\nuItemDF=uItemDF.select(col(\"_c0\").alias(\"movie_id\"),col(\"_c1\").alias(\"movie_title\"),col(\"_c2\").alias(\"release_date\"),col(\"_c3\").alias(\"video_release_date\"),col(\"_c4\").alias(\"imd_url\"),col(\"_c5\").alias(\"genre\"))\nuItemDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"829d780b-9c33-4bfa-a46e-a4538b4cc5c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|         movie_title|release_date|video_release_date|             imd_url|genre|\n+--------+--------------------+------------+------------------+--------------------+-----+\n|       1|    Toy Story (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       2|    GoldenEye (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       3|   Four Rooms (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       4|   Get Shorty (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       5|      Copycat (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       6|Shanghai Triad (Y...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       7|Twelve Monkeys (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       8|         Babe (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       9|Dead Man Walking ...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      10|  Richard III (1995)| 22-Jan-1996|              null|http://us.imdb.co...|    0|\n|      11|Seven (Se7en) (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      12|Usual Suspects, T...| 14-Aug-1995|              null|http://us.imdb.co...|    0|\n|      13|Mighty Aphrodite ...| 30-Oct-1995|              null|http://us.imdb.co...|    0|\n|      14|  Postino, Il (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|      15|Mr. Holland's Opu...| 29-Jan-1996|              null|http://us.imdb.co...|    0|\n|      16|French Twist (Gaz...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      17|From Dusk Till Da...| 05-Feb-1996|              null|http://us.imdb.co...|    0|\n|      18|White Balloon, Th...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      19|Antonia's Line (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      20|Angels and Insect...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n+--------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|         movie_title|release_date|video_release_date|             imd_url|genre|\n+--------+--------------------+------------+------------------+--------------------+-----+\n|       1|    Toy Story (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       2|    GoldenEye (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       3|   Four Rooms (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       4|   Get Shorty (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       5|      Copycat (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       6|Shanghai Triad (Y...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       7|Twelve Monkeys (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       8|         Babe (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|       9|Dead Man Walking ...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      10|  Richard III (1995)| 22-Jan-1996|              null|http://us.imdb.co...|    0|\n|      11|Seven (Se7en) (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      12|Usual Suspects, T...| 14-Aug-1995|              null|http://us.imdb.co...|    0|\n|      13|Mighty Aphrodite ...| 30-Oct-1995|              null|http://us.imdb.co...|    0|\n|      14|  Postino, Il (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|      15|Mr. Holland's Opu...| 29-Jan-1996|              null|http://us.imdb.co...|    0|\n|      16|French Twist (Gaz...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      17|From Dusk Till Da...| 05-Feb-1996|              null|http://us.imdb.co...|    0|\n|      18|White Balloon, Th...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      19|Antonia's Line (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      20|Angels and Insect...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n+--------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 10:  Code Challenge - Perform a Join\n\nUse Apache Spark to do the following:\n- join `uDataDF` and `uItemDf` on `movie_id` as a new DataFrame called `uMovieDF`  \n   **note:** make sure you do not create duplicate `movie_id` columns\n   \n**NOTE:** Please display the DataFrame `uMovieDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"686d417e-2180-43a7-8785-66abb3a9fcf9"}}},{"cell_type":"code","source":["uMovieDF=uDataDF.join(uItemDF,['movie_id'])\nuMovieDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c405db-dd36-46ca-9ebc-6e811d53c3ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|user_id|rating|   newdate|         movie_title|release_date|video_release_date|             imd_url|genre|\n+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|     242|    196|     3|1997-12-04|        Kolya (1996)| 24-Jan-1997|              null|http://us.imdb.co...|    0|\n|     302|    186|     3|1998-04-04|L.A. Confidential...| 01-Jan-1997|              null|http://us.imdb.co...|    0|\n|     377|     22|     1|1997-11-07| Heavyweights (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|      51|    244|     2|1997-11-27|Legends of the Fa...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     346|    166|     1|1998-02-02| Jackie Brown (1997)| 01-Jan-1997|              null|http://us.imdb.co...|    0|\n|     474|    298|     4|1998-01-07|Dr. Strangelove o...| 01-Jan-1963|              null|http://us.imdb.co...|    0|\n|     265|    115|     2|1997-12-03|Hunt for Red Octo...| 01-Jan-1990|              null|http://us.imdb.co...|    0|\n|     465|    253|     5|1998-04-03|Jungle Book, The ...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     451|    305|     3|1998-02-01|       Grease (1978)| 01-Jan-1978|              null|http://us.imdb.co...|    0|\n|      86|      6|     3|1997-12-31|Remains of the Da...| 01-Jan-1993|              null|http://us.imdb.co...|    0|\n|     257|     62|     2|1997-11-12| Men in Black (1997)| 04-Jul-1997|              null|http://us.imdb.co...|    0|\n|    1014|    286|     5|1997-11-17|Romy and Michele'...| 25-Apr-1997|              null|http://us.imdb.co...|    0|\n|     222|    200|     5|1997-10-05|Star Trek: First ...| 22-Nov-1996|              null|http://us.imdb.co...|    0|\n|      40|    210|     3|1998-03-27|To Wong Foo, Than...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      29|    224|     3|1998-02-21|Batman Forever (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|     785|    303|     3|1997-11-14|     Only You (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     387|    122|     5|1997-11-11|Age of Innocence,...| 01-Jan-1993|              null|http://us.imdb.co...|    0|\n|     274|    194|     2|1997-11-14|      Sabrina (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|    1042|    291|     4|1997-09-21|   Just Cause (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|    1184|    234|     2|1998-04-08|Endless Summer 2,...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|movie_id|user_id|rating|   newdate|         movie_title|release_date|video_release_date|             imd_url|genre|\n+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\n|     242|    196|     3|1997-12-04|        Kolya (1996)| 24-Jan-1997|              null|http://us.imdb.co...|    0|\n|     302|    186|     3|1998-04-04|L.A. Confidential...| 01-Jan-1997|              null|http://us.imdb.co...|    0|\n|     377|     22|     1|1997-11-07| Heavyweights (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|      51|    244|     2|1997-11-27|Legends of the Fa...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     346|    166|     1|1998-02-02| Jackie Brown (1997)| 01-Jan-1997|              null|http://us.imdb.co...|    0|\n|     474|    298|     4|1998-01-07|Dr. Strangelove o...| 01-Jan-1963|              null|http://us.imdb.co...|    0|\n|     265|    115|     2|1997-12-03|Hunt for Red Octo...| 01-Jan-1990|              null|http://us.imdb.co...|    0|\n|     465|    253|     5|1998-04-03|Jungle Book, The ...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     451|    305|     3|1998-02-01|       Grease (1978)| 01-Jan-1978|              null|http://us.imdb.co...|    0|\n|      86|      6|     3|1997-12-31|Remains of the Da...| 01-Jan-1993|              null|http://us.imdb.co...|    0|\n|     257|     62|     2|1997-11-12| Men in Black (1997)| 04-Jul-1997|              null|http://us.imdb.co...|    0|\n|    1014|    286|     5|1997-11-17|Romy and Michele'...| 25-Apr-1997|              null|http://us.imdb.co...|    0|\n|     222|    200|     5|1997-10-05|Star Trek: First ...| 22-Nov-1996|              null|http://us.imdb.co...|    0|\n|      40|    210|     3|1998-03-27|To Wong Foo, Than...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|      29|    224|     3|1998-02-21|Batman Forever (1...| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|     785|    303|     3|1997-11-14|     Only You (1994)| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n|     387|    122|     5|1997-11-11|Age of Innocence,...| 01-Jan-1993|              null|http://us.imdb.co...|    0|\n|     274|    194|     2|1997-11-14|      Sabrina (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|    1042|    291|     4|1997-09-21|   Just Cause (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n|    1184|    234|     2|1998-04-08|Endless Summer 2,...| 01-Jan-1994|              null|http://us.imdb.co...|    0|\n+--------+-------+------+----------+--------------------+------------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Question 11:  Code Challenge - Perform an Aggregation\n\nUse Apache Spark to do the following:\n1. create an aggregate DataFrame, `aggDF` by\n  1. extracting the year from the `date` (of the review)\n  1. getting the average rating of each film per year as a column named `average_rating`\n  1. ordering descending by year and average rating\n1. write the resulting dataframe to a table named \"movie_by_year_average_rating\" in the Default database  \n   **note:** use `mode(overwrite)` \n\n#### Desired Schema\nThe schema of you resulting DataFrame should be:\n- `year INTEGER`\n- `movie_title STRING`\n- `average_rating DOUBLE`\n\n**NOTE:** Please display the DataFrame `aggDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f033344-61ea-4ea7-a51d-ebaf22be8e62"}}},{"cell_type":"code","source":["from pyspark.sql.functions import year\nfrom pyspark.sql.window import Window\nuMovieDF1 = uMovieDF.withColumn(\"year\",year(\"newdate\"))\n\n\nwindowSpec = Window.partitionBy(uMovieDF1['year'])\nwindowSpec.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dee359d-24c7-4ee9-b6a0-2b582d516690"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4475868478686193>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mwindowSpec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muMovieDF1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'year'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mwindowSpec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m: 'WindowSpec' object has no attribute 'show'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'WindowSpec' object has no attribute 'show'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-4475868478686193>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mwindowSpec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muMovieDF1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'year'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mwindowSpec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m: 'WindowSpec' object has no attribute 'show'"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import year\nfrom pyspark.sql.window import Window\nuMovieDF1 = uMovieDF.withColumn(\"year\",year(\"newdate\"))\n\n\nwindowSpec = Window.partitionBy(uMovieDF1['year'])\ncols = [\"year\",\"movie_title\"]\n\n\nuMovieDF1.withColumn('average_rating', avg(\"rating\").over(Window.partitionBy(cols))).drop(\"movie_id\",\"user_id\",\"rating\",\"newdate\",\"release_date\",\"video_release_date\",\"imdb_url\",\"genre\").distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"565e6948-92f1-4c25-a215-78c14faa8c8e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+--------------------+----+------------------+\n|         movie_title|             imd_url|year|    average_rating|\n+--------------------+--------------------+----+------------------+\n|'Til There Was Yo...|http://us.imdb.co...|1997|               1.5|\n|        1-900 (1994)|http://us.imdb.co...|1997|               3.0|\n|101 Dalmatians (1...|http://us.imdb.co...|1997|2.8358208955223883|\n| 12 Angry Men (1957)|http://us.imdb.co...|1997| 4.295774647887324|\n|          187 (1997)|http://us.imdb.co...|1997| 2.857142857142857|\n|2 Days in the Val...|http://us.imdb.co...|1997|              3.25|\n|20,000 Leagues Un...|http://us.imdb.co...|1997|3.4634146341463414|\n|2001: A Space Ody...|http://us.imdb.co...|1997|               4.0|\n|39 Steps, The (1935)|http://us.imdb.co...|1997| 3.896551724137931|\n|        8 1/2 (1963)|http://us.imdb.co...|1997|              3.75|\n|8 Heads in a Duff...|http://us.imdb.co...|1997|               1.0|\n|    8 Seconds (1994)|http://us.imdb.co...|1997|               3.0|\n|A Chef in Love (1...|http://us.imdb.co...|1997|               4.2|\n|Above the Rim (1994)|http://us.imdb.co...|1997|               3.0|\n|Absolute Power (1...|http://us.imdb.co...|1997|3.3768115942028984|\n|   Abyss, The (1989)|http://us.imdb.co...|1997|3.6666666666666665|\n|Ace Ventura: Pet ...|http://us.imdb.co...|1997|3.0350877192982457|\n|Ace Ventura: When...|http://us.imdb.co...|1997|2.6315789473684212|\n|Across the Sea of...|http://us.imdb.co...|1997|               3.0|\n|Addams Family Val...|http://us.imdb.co...|1997| 2.772727272727273|\n+--------------------+--------------------+----+------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+--------------------+----+------------------+\n|         movie_title|             imd_url|year|    average_rating|\n+--------------------+--------------------+----+------------------+\n|'Til There Was Yo...|http://us.imdb.co...|1997|               1.5|\n|        1-900 (1994)|http://us.imdb.co...|1997|               3.0|\n|101 Dalmatians (1...|http://us.imdb.co...|1997|2.8358208955223883|\n| 12 Angry Men (1957)|http://us.imdb.co...|1997| 4.295774647887324|\n|          187 (1997)|http://us.imdb.co...|1997| 2.857142857142857|\n|2 Days in the Val...|http://us.imdb.co...|1997|              3.25|\n|20,000 Leagues Un...|http://us.imdb.co...|1997|3.4634146341463414|\n|2001: A Space Ody...|http://us.imdb.co...|1997|               4.0|\n|39 Steps, The (1935)|http://us.imdb.co...|1997| 3.896551724137931|\n|        8 1/2 (1963)|http://us.imdb.co...|1997|              3.75|\n|8 Heads in a Duff...|http://us.imdb.co...|1997|               1.0|\n|    8 Seconds (1994)|http://us.imdb.co...|1997|               3.0|\n|A Chef in Love (1...|http://us.imdb.co...|1997|               4.2|\n|Above the Rim (1994)|http://us.imdb.co...|1997|               3.0|\n|Absolute Power (1...|http://us.imdb.co...|1997|3.3768115942028984|\n|   Abyss, The (1989)|http://us.imdb.co...|1997|3.6666666666666665|\n|Ace Ventura: Pet ...|http://us.imdb.co...|1997|3.0350877192982457|\n|Ace Ventura: When...|http://us.imdb.co...|1997|2.6315789473684212|\n|Across the Sea of...|http://us.imdb.co...|1997|               3.0|\n|Addams Family Val...|http://us.imdb.co...|1997| 2.772727272727273|\n+--------------------+--------------------+----+------------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 4: Fun with JSON\n\nJSON values are typically passed by message brokers such as Kafka or Kinesis in a string encoding. When consumed by a Spark Structured Streaming application, this json must be converted into a nested object in order to be used.\n\nBelow is a list of json strings that represents how data might be passed from a message broker.\n\n**Note:** Make sure to run the cell below to retrieve the sample data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c84485-ab71-4cb8-85bc-244494d2cb4d"}}},{"cell_type":"code","source":["%python\n\n\nsampleJson = [\n ('{\"user\":100, \"ips\" : [\"191.168.192.101\", \"191.168.192.103\", \"191.168.192.96\", \"191.168.192.99\"]}',), \n ('{\"user\":101, \"ips\" : [\"191.168.192.102\", \"191.168.192.105\", \"191.168.192.103\", \"191.168.192.107\"]}',), \n ('{\"user\":102, \"ips\" : [\"191.168.192.105\", \"191.168.192.101\", \"191.168.192.105\", \"191.168.192.107\"]}',), \n ('{\"user\":103, \"ips\" : [\"191.168.192.96\", \"191.168.192.100\", \"191.168.192.107\", \"191.168.192.101\"]}',), \n ('{\"user\":104, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.102\", \"191.168.192.99\"]}',), \n ('{\"user\":105, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.100\", \"191.168.192.96\"]}',), \n]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba6bf8b7-4348-4b81-bc5f-d3b3c940db55"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 12:  Code Challenge - Count the IPs\n\nUse any coding techniques known to you to parse this list of JSON strings to answer the following question:\n- how many occurrences of each IP address are in this list?\n\n#### Desired Output\nYour results should be this:\n\n\n| ip | count |\n|:-:|:-:|\n| `191.168.192.96` | `3` |\n| `191.168.192.99` | `6` |\n| `191.168.192.100` | `2` |\n| `191.168.192.101` | `3` |\n| `191.168.192.102` | `2` |\n| `191.168.192.103` | `2` |\n| `191.168.192.105` | `3` |\n| `191.168.192.107` | `3` |\n\n**NOTE:** The order of your results is not important."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e34335a-ad2d-4ace-b01c-6144a67afad2"}}},{"cell_type":"code","source":["from pyspark.sql.functions import explode\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\ndf1=spark.createDataFrame(sampleJson)\n\nsch=StructType([StructField('user',StringType(),False),StructField('ips',ArrayType(StringType()))])\ndf2=df1.withColumn(\"n\",from_json(col(\"_1\"),sch)).select(\"n.*\")\ndf1.withColumn(\"n\",from_json(col(\"_1\"),sch)).select(\"n.*\").printSchema()\ndf2.withColumn('ip',explode(\"ips\")).groupby('ip').agg(count(\"*\").alias(\"count\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"356e7fd8-d527-4e25-ac9f-2e006942b048"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user: string (nullable = true)\n |-- ips: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+---------------+-----+\n|             ip|count|\n+---------------+-----+\n| 191.168.192.96|    3|\n|191.168.192.101|    3|\n|191.168.192.103|    2|\n| 191.168.192.99|    6|\n|191.168.192.105|    3|\n|191.168.192.107|    3|\n|191.168.192.102|    2|\n|191.168.192.100|    2|\n+---------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user: string (nullable = true)\n |-- ips: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+---------------+-----+\n|             ip|count|\n+---------------+-----+\n| 191.168.192.96|    3|\n|191.168.192.101|    3|\n|191.168.192.103|    2|\n| 191.168.192.99|    6|\n|191.168.192.105|    3|\n|191.168.192.107|    3|\n|191.168.192.102|    2|\n|191.168.192.100|    2|\n+---------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a295033-461b-482d-a3c6-571e507e6ca4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------+\n|            col|\n+---------------+\n|191.168.192.101|\n|191.168.192.103|\n| 191.168.192.96|\n| 191.168.192.99|\n|191.168.192.102|\n|191.168.192.105|\n|191.168.192.103|\n|191.168.192.107|\n|191.168.192.105|\n|191.168.192.101|\n|191.168.192.105|\n|191.168.192.107|\n| 191.168.192.96|\n|191.168.192.100|\n|191.168.192.107|\n|191.168.192.101|\n| 191.168.192.99|\n| 191.168.192.99|\n|191.168.192.102|\n| 191.168.192.99|\n+---------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------+\n|            col|\n+---------------+\n|191.168.192.101|\n|191.168.192.103|\n| 191.168.192.96|\n| 191.168.192.99|\n|191.168.192.102|\n|191.168.192.105|\n|191.168.192.103|\n|191.168.192.107|\n|191.168.192.105|\n|191.168.192.101|\n|191.168.192.105|\n|191.168.192.107|\n| 191.168.192.96|\n|191.168.192.100|\n|191.168.192.107|\n|191.168.192.101|\n| 191.168.192.99|\n| 191.168.192.99|\n|191.168.192.102|\n| 191.168.192.99|\n+---------------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Part 5: The Databricks API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57b8af12-a907-4c89-9ce4-1e6a9fecbcd9"}}},{"cell_type":"markdown","source":["### Question 13: Conceptual Question - the Databricks API\n\nIn 4-5 sentences, please explain what the Databricks API is used for at a high-level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2fcae3b-2d26-40d8-812e-83df5a2f402e"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cd0bbb2-0bb1-49f3-a002-2eef2a7096a7"}}},{"cell_type":"markdown","source":["### Question 14: Conceptual Question - Explain an API Call\n\nIn 4-5 sentences, please explain what this API call. Be sure to discuss some key attributes about the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0bfb77a-4f99-43c4-993a-7b5acb48db49"}}},{"cell_type":"markdown","source":["### Answer:\n\nThe databricks API can be used for multiple purposes\n\nClusters API is used for managing the databricks spark clusters e.g. create, edit,. start, restart,delete and other cluster lifecyle calls`\nDBFS API is used to interact with various data sources`\nGroups API is used to manage groups of users.`\nLikewise there are more API like Jobs, ML FLow,SCIM, Secrets, Libraries,Token ,Workspace to interact with various entities of Databricks`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24c8ad63-ed58-4704-91ef-95f2e99c0a60"}}},{"cell_type":"markdown","source":["## Part 6: Security"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"326ef8fd-bce0-46a2-b17e-65dbd1e687e7"}}},{"cell_type":"markdown","source":["### Question 15: Conceptual Question - Security on Databricks\n\n$ curl -n -X POST -H 'Content-Type: application/json'                      \\\n  -d '{                                                                     \\\n  \"cluster_name\": \"high-concurrency-cluster\",                               \\\n  \"spark_version\": \"4.2.x-scala2.11\",                                       \\\n  \"node_type_id\": \"i3.xlarge\",                                              \\\n  \"spark_conf\":{                                                            \\\n        \"spark.databricks.cluster.profile\":\"serverless\",                    \\\n        \"spark.databricks.repl.allowedLanguages\":\"sql,python,r\"             \\\n     },                                                                     \\\n     \"aws_attributes\":{                                                     \\\n        \"zone_id\":\"us-west-2c\",                                             \\\n        \"first_on_demand\":1,                                                \\\n        \"availability\":\"SPOT_WITH_FALLBACK\",                                \\\n        \"spot_bid_price_percent\":100                                        \\\n     },                                                                     \\\n   \"custom_tags\":{                                                          \\\n        \"ResourceClass\":\"Serverless\"                                        \\\n     },                                                                     \\\n       \"autoscale\":{                                                        \\\n        \"min_workers\":1,                                                    \\\n        \"max_workers\":2                                                     \\\n     },                                                                     \\\n  \"autotermination_minutes\":10                                              \\\n}' https://dogfood.staging.cloud.databricks.com/api/2.0/clusters/create '"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74d8a02c-08a3-4a58-9ec2-f493c4f22025"}}},{"cell_type":"markdown","source":["### Answer:\n\n`Network Isolation - create a dedicated VPC/VNET in the customer’s account that only contains Databricks infrastructure. This ensures that all Databricks policies and controls have no access to other production infrastructure in a customers account. Additionally, all communication with Databricks’ control infrastructure goes through direct links (while additionally eliminating any Public IPs). This prevents any traffic from traversing the public internet and is also encrypted with mutual TLS v1.2. And finally, multiple security groups are used so that if ever Databricks services need to connect to other customer VPC/VNETs (e.g., containing another production data source), there is no risk of exposing access to those services to the outside world.\n\nEnable SSO - Integration with SSO identity providers (such as Okta, Onelogin etc.) and support for SCIM allow you to easily manage and secure users.\n\nEnable ACLs - Cluster, Data, job, Workspace level\n\nEnable encryption at rest(storage level) and in motion(using custom code).`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b9a2086-89b1-469a-9719-acdbc0803339"}}},{"cell_type":"markdown","source":["# This is the end of the official test. Bonus below!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fefd4c92-ca46-4415-a4a4-e20b6743f52a"}}},{"cell_type":"markdown","source":["## Part 7: Bonus: Data Science & Machine Learning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8291308-a452-4735-9e42-70f17d69fe09"}}},{"cell_type":"markdown","source":["### Question 16: Conceptual Question - A Skewed Feature\n\nOne of these lines is the *mean* of this feature. The other is the *median*. Which of these lines is the **mean** - the red line or the black line?\n\n<img width=400px src=https://www.evernote.com/l/AAEycL6CQ0hLi5V5pIo91Ko-Pfk2i0AnGyMB/image.png>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51f7c7bb-9692-446a-89d5-53901b4bd0d8"}}},{"cell_type":"markdown","source":["### Answer:\n\n`The mean is always greater than the median, hence, red line is the median and black is the median.`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26e458c4-5872-4a00-ad3c-529894bde6eb"}}},{"cell_type":"markdown","source":["### Question 17: Conceptual Question - Exploratory Data Analysis\n\nThe plots below show the distribution of home selling prices differentiated by a few categorical features. Based on these plots, **which of these categorical features** - Property Type, Exterior Quality, or Month Sold - would you expect to be most associated with Price? **Why**?\n\n<img width=1600px src=https://www.evernote.com/l/AAHulkcc20hHSJV6D1udKiwSDCN0S6oV_5YB/image.png>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19932477-22f0-41ee-8b57-bd90fafcd515"}}},{"cell_type":"markdown","source":["### Answer:\n\nThe Exterior quality feature seems to be most associated with price as we can see the given graph"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f16f4fe6-828f-47ab-b3fc-3fd43132cac8"}}},{"cell_type":"markdown","source":["### Question 18: Conceptual Question - Analyze Model Performance\n\nConsider the following results for a decision tree model against training and testing data sets:\n\n`decision tree regression - train r2 score: 0.9944`  \n`decision tree regression - test r2 score:  0.3119`\n\n\nWhat is your assessment of this model?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e729420-5523-42c6-8688-ea0033cbbea9"}}},{"cell_type":"markdown","source":["### Answer:\nThe model is highly over-fitted because the training accuracy is very high as compared to the testing accuracy, we need to retrain the model and do cross validation techniques to verify our accuracy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351bd437-bc37-4ff5-8167-b72bf3500418"}}},{"cell_type":"markdown","source":["### Question 19: Conceptual Question - Model Selection\n\nA series of models has been built using the same training data, but each with a subset of features.\n\nConsider the following results for a series of logistic regression models and then answer this question:\n\n- Which model would you choose and why?\n- What other things would you want to look at and why?\n\n| model number | feature subset | logistic regression test accuracy|\n|:-:|:-:|:-:|\n| 1| feat_1 |\t 0.631|\n| 2| feat_2 |\t 0.552|\n| 3| feat_3 |\t 0.868|\n| 4| feat_4 |\t 0.868|\n| 5| feat_1, feat_2 |\t 0.657|\n| 6| feat_1, feat_3 |\t 0.947|\n| 7| feat_1, feat_4 |\t 0.921|\n| 8| feat_2, feat_3 |\t 0.947|\n| 9| feat_2, feat_4 |\t 0.973|\n| 10| feat_3, feat_4 |\t 0.947|\n| 11| feat_1, feat_2, feat_3 |\t 0.947|\n| 12| feat_1, feat_2, feat_4 |\t 0.947|\n| 13| feat_1, feat_3, feat_4 |\t 0.947|\n| 14| feat_2, feat_3, feat_4 |\t 0.973|\n| 15| feat_1, feat_2, feat_3, feat_4 |\t 0.973|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf31f496-31a5-4278-9dc5-2162056b2a9c"}}},{"cell_type":"markdown","source":["### Answer:\nI would chose model number 14 because it is giving highest accuracy and also it is better than model number 15 because it has an extra feature less which is not affecting the accuracy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13d0afaa-509b-4003-82a0-0e1b2622f1a8"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cdc6c4f-2425-4c2b-a991-6b4c42172d77"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"cse-take-home-assignment","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3725234820062616}},"nbformat":4,"nbformat_minor":0}
